{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp \n",
    "import numpy as np \n",
    "import cv2\n",
    "from keras.models import load_model, Model\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To make numpy arrays depending on emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "name = input(\"Enter the name of the data : \")\n",
    "\n",
    "holistic = mp.solutions.holistic\n",
    "hands = mp.solutions.hands\n",
    "holis = holistic.Holistic()\n",
    "drawing = mp.solutions.drawing_utils\n",
    "\n",
    "X = []\n",
    "data_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes in 1001 frames and then converts to .npy embedding file. This is done for every emotion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collects facial landmarks and stores them in a numpy array. These act as a base dataset here. For each emotion we have to collect the data. A facemesh and hand landmarks are generated and placed on the face and hands of the user and it shows how and what the model is capturing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\tlst = []\n",
    "\n",
    "\t_, frm = cap.read()\n",
    "\n",
    "\tfrm = cv2.flip(frm, 1)\n",
    "\n",
    "\tres = holis.process(cv2.cvtColor(frm, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "\tif res.face_landmarks:\n",
    "\t\tfor i in res.face_landmarks.landmark:\n",
    "\t\t\tlst.append(i.x - res.face_landmarks.landmark[1].x)\n",
    "\t\t\tlst.append(i.y - res.face_landmarks.landmark[1].y)\n",
    "\n",
    "\t\tif res.left_hand_landmarks:\n",
    "\t\t\tfor i in res.left_hand_landmarks.landmark:\n",
    "\t\t\t\tlst.append(i.x - res.left_hand_landmarks.landmark[8].x)\n",
    "\t\t\t\tlst.append(i.y - res.left_hand_landmarks.landmark[8].y)\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(42):\n",
    "\t\t\t\tlst.append(0.0)\n",
    "\n",
    "\t\tif res.right_hand_landmarks:\n",
    "\t\t\tfor i in res.right_hand_landmarks.landmark:\n",
    "\t\t\t\tlst.append(i.x - res.right_hand_landmarks.landmark[8].x)\n",
    "\t\t\t\tlst.append(i.y - res.right_hand_landmarks.landmark[8].y)\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(42):\n",
    "\t\t\t\tlst.append(0.0)\n",
    "\n",
    "\n",
    "\t\tX.append(lst)\n",
    "\t\tdata_size = data_size+1\n",
    "\n",
    "\n",
    "\n",
    "\tdrawing.draw_landmarks(frm, res.face_landmarks, holistic.FACEMESH_CONTOURS)\n",
    "\tdrawing.draw_landmarks(frm, res.left_hand_landmarks, hands.HAND_CONNECTIONS)\n",
    "\tdrawing.draw_landmarks(frm, res.right_hand_landmarks, hands.HAND_CONNECTIONS)\n",
    "\n",
    "\tcv2.putText(frm, str(data_size), (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),2)\n",
    "\n",
    "\tcv2.imshow(\"window\", frm)\n",
    "\n",
    "\tif cv2.waitKey(1) == 27 or data_size>1000:\n",
    "\t\tcv2.destroyAllWindows()\n",
    "\t\tcap.release()\n",
    "\t\tbreak\n",
    "\n",
    "np.save(f\"{name}.npy\", np.array(X))\n",
    "print(np.array(X).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_init = False\n",
    "size = -1\n",
    "\n",
    "label = []\n",
    "dictionary = {}\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Concentration of data\n",
    "\n",
    "* Basically finds the number of .npy files in the folder and will set number of parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir():\n",
    "    if i.split(\".\")[-1] == \"npy\" and not(i.split(\".\")[0] == \"labels\"):  \n",
    "        if not(is_init):\n",
    "            is_init = True \n",
    "            X = np.load(i)\n",
    "            size = X.shape[0]\n",
    "            y = np.array([i.split('.')[0]]*size).reshape(-1,1)\n",
    "        else:\n",
    "            X = np.concatenate((X, np.load(i)))\n",
    "            y = np.concatenate((y, np.array([i.split('.')[0]]*size).reshape(-1,1)))\n",
    "\n",
    "        label.append(i.split('.')[0])\n",
    "        dictionary[i.split('.')[0]] = c  \n",
    "        c = c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y.shape[0]):\n",
    "    y[i, 0] = dictionary[y[i, 0]]\n",
    "y = to_categorical(np.array(y, dtype=\"int32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X, y = X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "* I kept the training simple. It did take about 30-40 mins to train because of the CPU being used because tensorflow has stopped GPU compatibility for python 3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = Input(shape=(X.shape[1],))\n",
    "m = Dense(1024, activation=\"relu\")(ip)\n",
    "m = Dense(512, activation=\"relu\")(m)\n",
    "m = Dense(256, activation=\"relu\")(m)\n",
    "op = Dense(n_labels, activation=\"softmax\")(m)\n",
    "\n",
    "model = Model(inputs=ip, outputs=op)\n",
    "model.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\", metrics=['acc'])\n",
    "model.fit(X, y, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")\n",
    "np.save(\"labels.npy\", np.array(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/ Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = load_model(\"model.h5\")\n",
    "label = np.load(\"labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holistic = mp.solutions.holistic\n",
    "hands = mp.solutions.hands\n",
    "holis = holistic.Holistic()\n",
    "drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\tlst = []\n",
    "\n",
    "\t_, frm = cap.read()\n",
    "\n",
    "\tfrm = cv2.flip(frm, 1)\n",
    "\n",
    "\tres = holis.process(cv2.cvtColor(frm, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "\tif res.face_landmarks:\n",
    "\t\tfor i in res.face_landmarks.landmark:\n",
    "\t\t\tlst.append(i.x - res.face_landmarks.landmark[1].x)\n",
    "\t\t\tlst.append(i.y - res.face_landmarks.landmark[1].y)\n",
    "\n",
    "\t\tif res.left_hand_landmarks:\n",
    "\t\t\tfor i in res.left_hand_landmarks.landmark:\n",
    "\t\t\t\tlst.append(i.x - res.left_hand_landmarks.landmark[8].x)\n",
    "\t\t\t\tlst.append(i.y - res.left_hand_landmarks.landmark[8].y)\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(42):\n",
    "\t\t\t\tlst.append(0.0)\n",
    "\n",
    "\t\tif res.right_hand_landmarks:\n",
    "\t\t\tfor i in res.right_hand_landmarks.landmark:\n",
    "\t\t\t\tlst.append(i.x - res.right_hand_landmarks.landmark[8].x)\n",
    "\t\t\t\tlst.append(i.y - res.right_hand_landmarks.landmark[8].y)\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(42):\n",
    "\t\t\t\tlst.append(0.0)\n",
    "\n",
    "\t\tlst = np.array(lst).reshape(1,-1)\n",
    "\n",
    "\t\tpred = label[np.argmax(model.predict(lst))]\n",
    "\n",
    "\t\tprint(pred)\n",
    "\t\tcv2.putText(frm, pred, (50,50),cv2.FONT_ITALIC, 1, (255,0,0),2)\n",
    "\n",
    "\t\t\n",
    "\tdrawing.draw_landmarks(frm, res.face_landmarks, holistic.FACEMESH_CONTOURS)\n",
    "\tdrawing.draw_landmarks(frm, res.left_hand_landmarks, hands.HAND_CONNECTIONS)\n",
    "\tdrawing.draw_landmarks(frm, res.right_hand_landmarks, hands.HAND_CONNECTIONS)\n",
    "\n",
    "\tcv2.imshow(\"window\", frm)\n",
    "\n",
    "\tif cv2.waitKey(1) == 27:\n",
    "\t\tcv2.destroyAllWindows()\n",
    "\t\tcap.release()\n",
    "\t\tbreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
